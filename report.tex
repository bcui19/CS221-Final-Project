\documentclass[12pt]{scrartcl}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[top=1in, bottom=1in, left=0.8in, right=1in]{geometry}
\usepackage{enumerate}
\usepackage{courier}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{graphicx}
\renewcommand{\labelenumii}{\roman{enumii}}
\renewcommand{\qedsymbol}{\rule{0.7em}{0.7em}}
\newcommand{\tab}{\phantom{-----}}

\graphicspath{ {Images/} }


\setlength{\columnsep}{2.2em}
\linespread{1}

\title{Comparative Analysis of Artificial Intelligence Techniques for Cancer Diagnostics}
\author{Udai Baiswala - udai, \\ 
Brandon Cui - bcui19, \\ 
Natalie Ng - nng1}
\date{December 16, 2016}
\begin{document}
  \maketitle

  \vspace{-0.3in}
  \rule{\linewidth}{0.4pt}
  \hypersetup{%
    pdfborder = {0 0 0}
}
  
% ##################################################################################
%\begin{multicols*}{2}
    \section{Background}
    Microarrays are a high throughput lab-on-a-chip technology that enable measurement of DNA, RNA, and protein levels. With recent advances in microarrays, it is possible to relatively easily get data on the expression of at least 20,000 genes simultaneously. This is a powerful tool in biological research, but one which we believe can be better used. 
    
    In order to take advantage of the datasets provided by microarrays, we need to understand the performance of different analysis techniques on this data format. Typically, scientific labs use non-AI techniques to analyze microarrays. We hope to add AI techniques to the toolkit of analysis pipelines for microarray data. To do this, we hope to investigate the performance of various AI techniques on analyzing microarray data. 
    
    In order to perform this analysis, we need to first get datasets on which to run those AI techniques. In 2000, the National Center for Biotechnology Information established a public repository for the storage of high-throughput biological data called the Gene Expression Omnibus (GEO). To date, scientists have contributed over 4000 datasets with a total of over 2 million samples to GEO. The vast majority of this data was collected via microarrays. In our project, we have narrowed the scope by looking at datasets from GEO which measure cDNA expression levels using DNA microarrays. 

    \section{Task Definition}    
    To further narrow the scope of our project, we focus on the application of AI techniques for binary classification problems. After a quick skim of GEO, we notice that the vast majority of the datasets focused on binary classification problems are classifying with relation to cancer. Thus, our project focuses on binary classification problems in cancer research. These include classifying tissue between cancerous and benign, distinguishing patients between those that relapse and those that do not, and diagnosing cancer from circulating blood cells. 
    \subsection{Baseline and Oracle}
Our baseline is an implementation of K Nearest Neighbors. Our oracle is the false positive and false negative rates of current FDA approved cancer screens. Most of these screens range from 5\% to 20\% false positive rates, and 5\% to 10\% false negative rates. 
    \section{Literature Review}
    Current confirms the need for a comparative study of AI techniques in cancer classification problems. In 2006, JA Cruz et. al. report that preliminary machine learning techniques for cancer classification problems improves prediction accuracy by 10-15\% compared to traditional statistical methods. Thus, by gaining a deeper understanding of the benefits and drawbacks of different techniques, we can improve the accuracy of AI techniques by a higher margin.
    
    In 2015, K.Korou et. al. reports that machine learning techniques have been used for binary classification problems related to cancer, but additional validation of these techniques is needed. Our project provides a first step in the validation of such techniques in DNA microarray data.
    \section{Infrastructure}
    We collect our datasets from the Gene Expression Omnibus. These datasets are typically annotated with a chip-specific probe name for a particular gene. We convert these probe names to universal gene names to enable comparison between different datasets. 
    \subsection{Datasets}
    \subsubsection{GSE16449}
    This dataset contains 70 individuals with 34,731 gene expressions each. In addition, the dataset contains information of whether the sample comes from cancerous or benign kidney tissue. We use this dataset for the binary classification problem of cancerous versus benign.
    \subsubsection{GSE13576}
    This dataset contains 196 patients with 55,670 gene expressions each. In addition, the dataset contains information of whether the . We use this dataset for the binary classification problem of cancerous versus benign.

    \subsection{Normalization}
Microarray data has variation in the amount of each sample inputted into the chip. This leads to a systematic error where one sample has greater expression across all the genes. Thus, we apply a normalization step to each sample. To do this, we sum the expression of the genes for each sample. We then divide each expression by this sum. This scales all the samples to have the same total expression, reducing  noise in the experiment.
    \section{Methods}
    Our methodology consists of running K Nearest Neighbors (KNN) for our baseline and then we implement logistic regression, stochastic gradient descent (SGD), support vector machines (SVMs), Multi-layer neural networks, and Long Short Term Memory (LSTMs).
    
    When training all models, we run 10-fold cross validation. We present the testing error for all such samples. 

    \subsection{Sci-Kit Learn}
    \subsubsection*{5.5.1 K Nearest Neighbors}
    We run 5-NN on our dataset with the KNN package from scikit-learn. We run the algorithm with a ball tree in order to get around deficiencies in KD-trees and brute force distances. 
    
    \subsubsection*{5.1.2 Logistic Regression}
    For our project we use logistic regression as the baseline for our projection. The logistic regression implementation is performed with the class weight flag turned onto 'balanced'. Thus the weight that each class adds in training is multiplied by the following constant:
    
    $$\frac{\textrm{num samples}}{\textrm{num classInstances}}$$
    
    Thus the weights within logistic regression are adjusted inversely proportional to the class frequencies. Overall, this allows for better classification, particularly with our datasets that are biased towards non-cancerous samples. 
    \subsubsection*{5.1.3 Support Vector Machines SVMs}
    We apply SVMs from the sklearn package with the same flag as in logistic regression, where every class was weighted by multiplying it by the constant
    
    $$\frac{\textrm{num samples}}{\textrm{num classInstances}}$$
    
    \subsubsection*{5.1.4 Stochastic Gradient Descent}
    We also use stochastic gradient descent as a means to classify the datasets. Our implementation utilizes hinge loss, an elasticnet penalty, and a balanced class weight. The elasticnet parameter serves to try to optimize the combination of $l1$ and $l2$ as follows:
    
    $$(1-l1ratio)*L2 +l1ratio*L1$$
    
    where the $l1ratio$ is an initialized input constant that defaults to $0.15$ and $L1$ represents the $L1$ norm and $L2$ represents the $L2$ norm. In general we choose to use elastic net in order to try to reduce the number of features, since if we were to just use L1 as the penalty then it'd drive nearly all of the parameters to $0$, giving us a sparse solution. Thus, with elastic net we try to get a set of solutions that isn't sparse.
    
    \subsection{Tensor Flow}
    fill me in (Brandon)

    \subsection{Preprocessing}
We notice that our data has a high feature to sample ratio. We test the efficacy of a preprocessing step in minimizing overfitting and improving runtime efficiency. Our preprocessing step includes an initial filtration of data to restrict model training to important features that likely have biological relevance. We implement the following two filtration techniques.

    \subsubsection{Differential Expression}
We only include genes that are differentially expressed between the two classifications. We test for significance using a Welch t-test in the sci-py package with a significance cutoff of 0.01. 
    \subsubsection{Fold Change}
We ensure a minimum fold-change of 5. We tested different amounts of fold change and settled on a number that would enable us to retain approximately 10\% of the features in model training.

    \section{Results}
    \subsection{GSE16449: Kidney Cancer Prediction}
    We begin by attempting a binary classification problem between cancerous and benign kidney tissue. 
    \subsubsection{Prediction Accuracy}
    The results below are when running our 5-NN algorithm on the Kidney Cancer dataset (table 1):
   \begin{center}
   \begin{tabular}{c|c|c|c}
   \hline
   & Correctly Predicted & Actual & Percent\\
   \hline
   Cancerous & 18 & 18 & 100\\
   Benign & 52 & 52 & 0\\
   \hline
   \end{tabular}\\
   \vspace{0.1in}
   \textbf{Table 1: 5-NN without Filtration}
    \end{center}
    
    We present the results when running logistic regression on the Kidney Cancer dataset (table 2):
    
    \begin{center}
    \begin{tabular}{c|c|c|c}
    \hline
    & Correctly Predicted & Actual & Percent\\
    \hline
    Cancerous & 16 & 18 & 88.89\\
    Benign & 49 & 52 & 94.23\\
    \hline
    \end{tabular}\\
    \vspace{0.1in}
    \textbf{Table 2: Logistic Regression without Filtration}
    \end{center}
    
    We show the results when running SVMs on the Kidney Cancer dataset (table 3):
    
    \begin{center}
    \begin{tabular}{c|c|c|c}
    \hline
    & Correctly Predicted & Actual & Percent\\
    \hline
    Cancerous & 15 & 18 & 83.3\\
    Benign & 52 & 52 & 100\\
    \hline
    \end{tabular}\\
    \vspace{0.1in}
    \textbf{Table 3: SVM without Filtration}
    \end{center}
    
    We present the results when running 5-NN, Logistic Regression, and SVM on the filtered datasets below:
    
    \begin{center}
    \begin{tabular}{c|c|c|c}
    \hline
    & Correctly Predicted & Actual & Percent\\
    \hline
    Cancerous & 18 & 18 & 100\\
    Benign & 52 & 52 & 0\\
    \hline
    \end{tabular}\\
    \vspace{0.1in}
    \textbf{Table 4: 5-NN with Filtration}
    \end{center}
    
    \begin{center}
    \begin{tabular}{c|c|c|c}
    \hline
    & Correctly Predicted & Actual & Percent\\
    \hline
    Cancerous & 16 & 18 & 88.89\\
    Benign & 52 & 52 & 100\\
    \hline
    \end{tabular}\\
    \vspace{0.1in}
    \textbf{Table 5: Logreg with Filtration}
    \end{center}
    
    \begin{center}
    \begin{tabular}{c|c|c|c}
    \hline
    & Correctly Predicted & Actual & Percent\\
    \hline
    Cancerous & 16 & 18 & 88.89\\
    Benign & 52 & 52 & 100\\
    \hline
    \end{tabular}\\
    \vspace{0.1in}
    \textbf{Table 6: SVM with Filtration}
    \end{center}
    
    
    
    
    \subsubsection{Runtime}
    Due to our preprocessing methods we are able to run all of our methods with a smaller feature set, as a result we are able to see a drastic improvement in runtime. The graph below is the amount of time it takes to load the data and train and test for all 10 folds (Figure 1):
    
    \begin{center}
    \includegraphics[width = 5in]{runTimeCancer}\\
    \textbf{Figure 1: Runtimes for Different methods on the GSE16449 Dataset}
    \end{center}
    
    \subsubsection{Preliminary Error Analysis}
    We realized that perhaps the classification problem of cancerous versus benign is too trivial. This makes it very difficult to compare the results from different AI techniques, as all techniques will perform reasonably well. We also realize that our dataset does not contain enough samples to do proper training and validation of the model. Thus, in later analysis, we choose harder classification problems with larger datasets.
    
    \subsection{GSE13576: Relapse Prediction Dataset}
    We choose a different dataset with the aim of tackling a harder classification problem. With this dataset we attempt to analyze whether or not a series of genes will lead to relapse in cancer. For this we filtered with two different cutoffs, fold change = 5 and fold change = 10, this resulted in 2444 and 557 genes, respectively. 
    
    \subsubsection{Results}    
    \begin{center}
    \includegraphics[width = 5in]{classificationRemissionHeavilyFiltered}\\
    \textbf{Figure 2: Classification Accuracy}
    \end{center}
   
    From the classification results we notice that the filtration of features helps us ensure that the classifier does not classify everything as relapse (Figure 2). Additionally, we notice that an SVM in both the unfiltered and the filtered case enables the balance of false positive and false negative rates. However, we also note that when we ran the machine learning algorithms on an  more heavily filtered dataset that the results didn't improve significantly; we did notice that we got a slight increase in the true negative rate and a slight decrease in the true positive rate. Thus, we conclude that there is an asymptotic ceiling to the benefits of filtering. 
   
    \subsubsection{Runtime}
    We observe that the runtimes (Figure 3) for the algorithms on the same-sized dataset lie within one order of magnitude. However, when we compare between the datasets, we notice that the unfiltered dataset performs at  an order of magnitude worse than the filtered dataset. Thus, filtration provides an additional benefit of increased runtime efficiency.
    
    \begin{center}
    \includegraphics[width = 5in]{runTimeRemission}\\
    \textbf{Figure 3: Runtimes for different methods on the GSE 13576 Dataset}
    \end{center}
    
    
    
    \subsection{Precision and Recall}
    The precision and recall for the various methodologies are presented below:
    \begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
    method & precision & recall\\
    \hline
    SVM - heavily filtered & 0.4 & 0.8125\\
    SGD - heavily filtered & 0.339285714 & 0.59375\\
    Logistic Regression - heavily filtered & 0.5 & 0.59375\\
    KNN - heavily filtered & 0.355932203 & 0.65625\\
    SVM - filtered & 0.338235294 & 0.71875\\
    SGD - filtered & 0.301587302    & 0.59375\\
    Logistic Regression - filtered & 0.515151515 & 0.53125\\
    KNN - filtered & 0.375 & 0.46875\\
    SVM - all & 0.311111111 & 0.4375\\
    SGD - all & 0.13253 & 0.6875\\
    Logistic regression - all & 0.375 & 0.1875\\
    KNN - all & 0.25 & 0.0625\\
    
    \hline
    \end{tabular}\\
    \vspace{0.1in}
    \textbf{Table 7: Precision and Recall for various methodologies}  
    \end{center}
    
    We notice that for all the AI techniques, the precision of the test is quite poor. This signifies that our methods declare a large number of non-relapse samples as likely to develop relapse. For the purposes of a diagnostic test, and assuming a tradeoff between false positive and false negative rates, the current situation is better than the alternative, where false negatives are far more troublesome than false positives. Additionally, we notice that filtration provides an improvement on both precision and recall, signifying the importance of using genes that are likely to have biological relevance. We also notice that SVM methods provide the best recall.
    
    To better gauge the tradeoff between false positive and false negative rates for each technique, we calculate the F1 score (Figure 4).
    
    \begin{center}
    \includegraphics[width = 5in]{F1_Scores}\\
    \textbf{Figure 4: F1 scores for various methodologies}
    \end{center}

    Based upon the F1 scores, we know that running an SVM with the heavily filtered dataset we will get the best results in terms of F1 scores. Additionally, in general we notice that an SVM will consistently produce similar F1 scores. 


    \section{Error Analysis}
    From the results, we notice that our baseline consistently does the worst. This is sensible since KNN performance suffers because of the curse of dimensionality. Our implementation of stochastic gradient descent attempts to further reduce our feature space by using elastic net. However, with our implementation of SGD we didn't normalize the features to -1, 1 rather they are some float representing the weight of a feature, and because SGD is sensitive to feature scaling, we believe that there is a possible intrinsic issue. Additionally, SGD is generally trained on a much larger dataset, and since we use a very small training set it is very hard to fit a good model using SGD. \\
    
    We notice that the F1 score for logistic regression is the best on the filtered dataset. Since the difference between the two models is that logistic regression tries to create a probabilistic model, while SVMs attempt to create a hyperplane that maximizes the margin between the two classes, then because the SVM is operating in a high dimensional space and we are working with very little training data that the SVM can overfit more than logistic regression on the filtered data set. However, when we further try to reduce the noise with the heavily filtered dataset we are able to achieve similar F1 values for logistic regression and SVM. Overall, we believe that one way to improve our results would be to add more training data to try to reduce overfitting, but also we notice that by reducing the dimensionality of the data we already receive drastic improvements in the F1 score since we are statistically removing noise from our dataset. 
    
    

    
    
    \begin{thebibliography}{9}

\bibitem{obnibus}
Barrett, Tanya. "Gene Expression Omnibus (GEO)." \textit{The NCBI Handbook [Internet]}. 2nd Edition. U.S. National Library of Medicine, 2013. Web. 14 Dec. 2016.

\bibitem{cruz}
Cruz, Joseph A., and David S. Wishart. "Applications of Machine Learning in Cancer Prediction and Prognosis." \textit{Cancer Informatics} (2006): 59-77. \textit{US National Library of Medicine}. Web. 14 Dec. 2016.

\bibitem{kourou}
Kourou, Konstantina, Themis Exarchos, and Konstantinos Exarchos. "Machine Learning Applications in Cancer Prognosis and Prediction." \textit{Computational and Structural Biotechnology Journal} 13 (2015): 8-17. Web. 14 Dec. 2016.

\end{thebibliography}
    
%\end{multicols*}
\end{document}
