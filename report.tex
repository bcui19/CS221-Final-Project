\documentclass[12pt]{scrartcl}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[top=1in, bottom=1in, left=0.8in, right=1in]{geometry}
\usepackage{enumerate}
\usepackage{courier}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{graphicx}
\renewcommand{\labelenumii}{\roman{enumii}}
\renewcommand{\qedsymbol}{\rule{0.7em}{0.7em}}
\newcommand{\tab}{\phantom{-----}}

\graphicspath{ {Images/} }


\setlength{\columnsep}{2.2em}
\linespread{1}

\title{Comparative Analysis of Artificial Intelligence Techniques for Cancer Diagnostics}
\author{Udai Baiswala - udai, \\ 
Brandon Cui - bcui19, \\ 
Natalie Ng - nng1}
\date{December 16, 2016}
\begin{document}
  \maketitle

  \vspace{-0.3in}
  \rule{\linewidth}{0.4pt}
  \hypersetup{%
    pdfborder = {0 0 0}
}
  
% ##################################################################################
%\begin{multicols*}{2}
    \section{Background}
    Microarrays are a high throughput lab-on-a-chip technology that enable measurement of DNA, RNA, and protein levels. With recent advances in microarrays, it is possible to obtain the expression of at least 20,000 genes simultaneously. This is a powerful tool in biological research, but one which we believe can be better used. \\
    
    In order to take advantage of the datasets provided by microarrays, we need to understand the performance of different analysis techniques on this data format. Typically, scientific labs use non-AI techniques to analyze microarrays. We hope to add AI techniques to the toolkit of analysis pipelines for microarray data. To do this, we plan to investigate the performance of various AI techniques on analyzing microarray data. \\
    
    In order to perform this analysis, we need to first get datasets on which to run those AI techniques. In 2000, the National Center for Biotechnology Information established a public repository for the storage of high-throughput biological data called the Gene Expression Omnibus (GEO). To date, scientists have contributed over 4000 datasets with a total of over 2 million samples to GEO. The vast majority of this data was collected via microarrays. In our project, we narrow the scope by looking at datasets from GEO which measure cDNA expression levels using DNA microarrays. 

    \section{Task Definition}    
    To further narrow the scope of our project, we focus on the application of AI techniques for binary classification problems. After a quick skim of GEO, we notice that the vast majority of the datasets focus on binary classification problems related to cancer. Thus, our project focuses on binary classification problems in cancer research. These include classifying tissue between cancerous and benign tissue and distinguishing patients between those that relapse and those that do not. 
    \subsection{Baseline and Oracle}
Our baseline is an implementation of K Nearest Neighbors. Our oracle is the false positive and false negative rates of current FDA approved cancer screens. Most of these screens range from 5\% to 20\% false positive rates, and 5\% to 10\% false negative rates. Our baseline yields a 0\% false negative rate and a 100\% false positive rate when we attempt the same cancer detection problem (using the GSE16449 dataset). 

    \section{Literature Review}
    A literature review confirms the need for a comparative study of AI techniques in cancer classification problems. J.A. Cruz et. al. report that preliminary machine learning techniques for cancer classification problems improves prediction accuracy by 10-15\% compared to traditional statistical methods (2006). Thus, by gaining a deeper understanding of the benefits and drawbacks of different techniques, we can continue to improve the accuracy of the AI techniques used by researchers.
    
    K. Korou et. al. reports that machine learning techniques have been used for binary classification problems related to cancer, but additional validation of these techniques is needed (2015). Our project provides a first step in the validation of such techniques in DNA microarray data.
    \section{Infrastructure}
    We collect our datasets from the Gene Expression Omnibus. These datasets are typically annotated with a chip-specific probe name for a particular gene. We can convert these probe names to universal gene names to enable comparison between different datasets. 
    \subsection{Datasets}
    \subsubsection{GSE16449}
    This dataset contains 70 individuals with 34,731 gene expressions each. In addition, the dataset contains information on whether the sample comes from cancerous or benign kidney tissue. We use this dataset for the binary classification problem of cancerous versus benign.
    \subsubsection{GSE13576}
    This dataset contains 196 patients with 55,670 gene expressions each. In addition, the dataset contains information on whether that patient relapsed or not. We use this dataset for the binary classification problem of predicting whether the patient will relapse. 

    \subsection{Normalization}
Microarray data has variation in the amount of each sample inputted into the chip. This leads to a systematic error where one sample has greater expression across all the genes. Thus, we apply a normalization step to each sample. To do this, we sum the expression of the genes for each sample. We then divide each expression by this sum. This process scales all the samples so that they will have the same total expression, reducing  noise in the experiment.
    \section{Methods}
    Our methodology consists of running K Nearest Neighbors (KNN) for our baseline and then running logistic regression, stochastic gradient descent (SGD), support vector machines (SVMs), Multi-layer neural networks, and Long Short Term Memory (LSTMs) in order to evaluate relative accuracy on each dataset. \\
    
    When training all models, we run 10-fold cross validation. We present the testing error for all such samples. 

    \subsection{Sci-Kit Learn}
    \subsubsection*{5.5.1 K Nearest Neighbors}
    We run 5-NN on our dataset with the KNN package from scikit-learn. We run the algorithm with a ball tree in order to get around deficiencies in KD-trees and brute force distances. 
    
    \subsubsection*{5.1.2 Logistic Regression}
    We run logistic regression from scikit-learn, but with the class weight flag turned onto 'balanced'. Thus the weight that each sample adds in training is multiplied by the following constant:
    
    $$\frac{\textrm{total num samples}}{\textrm{num samples of this class}}$$
    
    This means that the weights within logistic regression are adjusted to a degree inversely proportional to the class frequencies during training. Specifically, the weights are adjusted more for samples of the rarer class. Since our datasets of choice have considerably more samples of one class (e.g. most samples are non-cancerous), this improves classification performance.
    
    \subsubsection*{5.1.3 Support Vector Machines SVMs}
    We apply SVMs from the scikit-learn package with the same flag as in logistic regression, where every class is weighted by multiplying the constant
    
    $$\frac{\textrm{num samples}}{\textrm{num samples of this class}}$$
    
    \subsubsection*{5.1.4 Stochastic Gradient Descent}
    We also use stochastic gradient descent as a means to classify the datasets. Our implementation utilizes hinge loss, an elasticnet penalty, and a balanced class weight. The elasticnet parameter serves to try to optimize the combination of $l1$ and $l2$ as follows:
    
    $$(1-l1ratio)*L2 +l1ratio*L1$$
    
    where the $l1ratio$ is an initialized input constant that defaults to $0.15$, $L1$ represents the $L1$ norm, and $L2$ represents the $L2$ norm. In general we use elastic net in order to reduce the number of features, since if we were to just use $L1$ as the penalty, that would drive nearly all of the parameters to $0$, giving us a sparse solution. Thus, with elastic net we try to get a set of solutions that isn't sparse.
    
    \subsubsection{Neural Networks}
    Additionally, we choose to apply neural networks to classify our data. Our implementation uses lbfgs algorithm which approximates Newton's method for weights that will be updated using backpropagation. The model also utilizes relu as the activation function. The neural network was a 4-hidden layer neural network with 7 hidden neurons per a layer. 
    
    
    \subsection{Tensor Flow: LSTM}
    We also implement a long short-term memory(LSTM) recurrent neural network (RNN). For our RNN our batch size is 3 and for each iteration of training we train over 31 timesteps, our embedding size is 19, and the RNN has 128 hidden units. 
 
    \subsection{Filtration: Preprocessing}
We notice that our data has a high feature to sample ratio. Consequently, we attempt to preprocess the data in order to minimize overfitting and improve runtime. Through preprocessing, we attempt to filter the data to restrict model training to features that are likely to have biological relevance. We implement the following two filtration techniques. 

    \subsubsection{Filtration: Differential Expression}
We only include genes that are differentially expressed between the two classifications. We test for significance using a Welch t-test in the sci-py package with a significance cutoff of 0.01. 
    \subsubsection{Fold Change}
We ensure a minimum fold-change of 5. We tested different cutoffs for fold change and settled on a number that would enable us to retain approximately 10\% of the features in model training.

    \subsubsection{Resampling the dataset}
    We also choose to implement another method of preprocessing to balance our datasets; this is done by resampling our datasets. We did this in two ways, by undersampling the dataset, thus removing some of the negative classifications or by oversampling by duplicating the positive classifications until we reach a balance between the positive and negative classifications. 


    \section{Results}
    \subsection{GSE16449: Kidney Cancer Prediction}
    As described earlier, we first attempt binary classification between cancerous and benign kidney tissue. 
    \subsubsection{Prediction Accuracy}
    The results below are when running our 5-NN algorithm on the Kidney Cancer dataset (table 1):
   \begin{center}
        
   \begin{tabular}{c|c|c|c}
   \hline
   & Correctly Predicted & Actual & Percent\\
   \hline
   Cancerous & 18 & 18 & 100\\
   Benign & 0 & 52 & 0\\
   \hline
   \end{tabular}\\
   \vspace{0.1in}
   \textbf{Table 1: 5-NN without Filtration}
    \end{center}
    
    We present the results when running logistic regression on the Kidney Cancer dataset (table 2):
    
    \begin{center}
        
    \begin{tabular}{c|c|c|c}
    \hline
    & Correctly Predicted & Actual & Percent\\
    \hline
    Cancerous & 16 & 18 & 88.89\\
    Benign & 49 & 52 & 94.23\\
    \hline
    \end{tabular}\\
    \vspace{0.1in}
    \textbf{Table 2: Logistic Regression without Filtration}
    \end{center}
    
    We show the results when running SVMs on the Kidney Cancer dataset (table 3):
    
    \begin{center}
    \begin{tabular}{c|c|c|c}
    \hline
    & Correctly Predicted & Actual & Percent\\
    \hline
    Cancerous & 15 & 18 & 83.3\\
    Benign & 52 & 52 & 100\\
    \hline
    \end{tabular}\\
    \vspace{0.1in}
    
    \textbf{Table 3: SVM without Filtration}
    \end{center}
    
    Finally, we present the results when running 5-NN, Logistic Regression, and SVM on the filtered datasets below:
    
    \begin{center}
    \begin{tabular}{c|c|c|c}
    \hline
    & Correctly Predicted & Actual & Percent\\
    \hline
    Cancerous & 18 & 18 & 100\\
    Benign & 52 & 52 & 0\\
    \hline
    \end{tabular}\\
    \vspace{0.1in}
    \textbf{Table 4: 5-NN with Filtration}
    \end{center}
    
    \begin{center}
    \begin{tabular}{c|c|c|c}
    \hline
    & Correctly Predicted & Actual & Percent\\
    \hline
    Cancerous & 16 & 18 & 88.89\\
    Benign & 52 & 52 & 100\\
    \hline
    \end{tabular}\\
    \vspace{0.1in}
    \textbf{Table 5: Logreg with Filtration}
    \end{center}
    
    \begin{center}
    \begin{tabular}{c|c|c|c}
    \hline
    & Correctly Predicted & Actual & Percent\\
    \hline
    Cancerous & 16 & 18 & 88.89\\
    Benign & 52 & 52 & 100\\
    \hline
    \end{tabular}\\
    \vspace{0.1in}
    \textbf{Table 6: SVM with Filtration}
    \end{center}
    
    
    
    
    \subsubsection{Runtime}
    With our filtration method, we are able to run our methods with a smaller feature set. We see that this has a dramatic improvement in runtime. In addition, from the above section, we notice that filtration does not compromise accuracy. The graph below is the amount of time it takes to load the data and train and test for all 10 folds (Figure 1):
    
    \begin{center}
    \includegraphics[width = 5in]{runTimeCancer}\\
    \textbf{Figure 1: Runtimes for Different methods on the GSE16449 Dataset}
    \end{center}
    
    \subsubsection{Error Analysis}
    We realized that perhaps the classification problem of cancerous versus benign is too trivial. This makes it very difficult to compare the results from different AI techniques, as all techniques will perform reasonably well. We also realize that our dataset does not contain enough samples to do proper training and validation of the model. Thus, in later analysis, we choose a harder classification problems and a alrger dataset.
    
    \subsection{GSE13576: Relapse Prediction Dataset}
    We choose a different dataset with the aim of tackling a harder classification problem. With this dataset, we attempt to distinguish which of genes will lead to relapse in cancer. For this we filtered with two different cutoffs, fold change = 5 and fold change = 10, this resulted in 2444 and 557 genes, respectively. Additionally, when we oversample the data we copy the data points that lead to remission leading to 161 data points that lead to relapse and 165 data points that do not, while when we undersample we remove a significant portion of the non-remission samples giving us 58 data points that do not lead to relapse and 57 data points that lead to relapse. 
    
    \subsubsection{Results}    
    \begin{center}
    \includegraphics[width = 5in]{classificationNeuralNets}\\
    \textbf{Figure 2: Classification Accuracy}
    \end{center}
   
    From the classification results we notice that the filtration of features helps us ensure that the classifier does not classify everything as relapse (Figure 2). We further note that by over or under sampling we can further improve our results, since we are working with a balanced dataset. However, we also note that when we run machine learning algorithms on a more heavily filtered dataset that the results do not improve significantly; we observe a slight increase in the true negative rate but a slight decrease in the true positive rate. Thus, we conclude that there is an asymptotic ceiling to the benefits of filtering. Lastly, we note that an LSTM on the oversampled dataset produces the best results with nearly perfect classification for relapse and non-relapse. 
    
    \subsubsection{LSTM Loss Over Time}
    We present the average loss per a fold over epochs in the LSTM below (figure 3):
    
    \begin{center}
    \includegraphics[width = 5in]{LSTM_Loss}\\
    \textbf{Figure 3: LSTM Loss Over Epoch}
    \end{center}
    
    We note that even after just a few epochs the loss drastically increases. Additionally, when oversampling the loss starts drops more than when undersampling the data, though at this point it is difficult to determine if this reflects the benefits of oversampling or if this observation represents noise or overfitting. 
    
       
    \subsubsection{Runtime}
    We observe that the runtimes (Figure 4) for "basic" AI techniques (not including neural nets or LSTMs) on the same-sized dataset lie within one order of magnitude. However, when we compare between the datasets, we notice that the unfiltered dataset performs an order of magnitude worse than the filtered dataset. Thus, filtration provides an additional benefit of increased runtime efficiency.
    
    \begin{center}
    \includegraphics[width = 5in]{runTimeRemission}\\
    \textbf{Figure 4: Runtimes for different methods on the GSE 13576 Dataset}
    \end{center}
    
    
    
    \subsection{Precision and Recall}
    The precision and recall for the various methodologies are presented in Table 7. We notice that for all the AI techniques, without oversampling or undersampling, the precision of the test is quite poor. This signifies that our methods declare a large number of non-relapse samples as likely to develop relapse. For the purposes of a diagnostic test, and assuming a tradeoff between false positive and false negative rates, having a low precision is not too bad, given that false negatives are far more troublesome than false positives. Additionally, when we filter the datasets we get improvements on both precision and recall, signifying the importance of using genes that are likely to have biological relevance and reducing the set of features in our models. We also observe that Neural Networks, LSTMs, and logistic regression provide the best precision and recall. \\
     
    Additionally, we note that we can remedy the low precision through resampling techniques. In the original dataset, relapse samples are severly over-represented. Therefore, it can be beneficial for model accuracy to declare all the samples as relapse. By oversampling or undersampling, we balance the representation of relapse and non-relapse samples, improving the training of the models.
    \begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
    method & precision & recall\\
    \hline
    LSTM - Oversampling & 0.9415 & 1\\
    LSTM - Undersampling & 0.6436 & 0.9824\\
    Neural net - Oversampling & 0.8229 & 0.98136\\
    Neural net - Undersampling & 0.7222 & 0.91228\\
    SVM - Oversampling & 0.8157 & 0.9627\\
    SVM - Undersampling & 0.7 & 0.8596\\
    LogReg - Oversampling & 0.8743 & 0.9937\\
    LogReg - Undersampling & 0.71798& 0.9824\\
    SVM - heavily filtered & 0.4 & 0.8125\\
    SGD - heavily filtered & 0.339285714 & 0.59375\\
    Logistic Regression - heavily filtered & 0.5 & 0.59375\\
    KNN - heavily filtered & 0.355932203 & 0.65625\\
    SVM - filtered & 0.338235294 & 0.71875\\
    SGD - filtered & 0.301587302    & 0.59375\\
    Logistic Regression - filtered & 0.515151515 & 0.53125\\
    KNN - filtered & 0.375 & 0.46875\\
    SVM - all & 0.311111111 & 0.4375\\
    SGD - all & 0.13253 & 0.6875\\
    Logistic regression - all & 0.375 & 0.1875\\
    KNN - all & 0.25 & 0.0625\\
    
    \hline
    \end{tabular}\\
    \vspace{0.1in}
    \textbf{Table 7: Precision and Recall for various methodologies}  
    \end{center}
    
    
    
    To better gauge the tradeoff between false positive and false negative rates for each technique, we calculate the F1 score (Figure 5).
    
    \begin{center}
    \includegraphics[width = 5in]{F1Scores}\\
    \textbf{Figure 5: F1 scores for various methodologies}
    \end{center}
    
    Looking at F1 scores, we note that by filtering the data set and utilizing resampling techniques (oversampling and undersampling), we can significantly improve the F1 score. We see slightly better results with LSTMs compared to more "basic" AI techniques, but this improvement is slight relative to the improvement from sampling and filtration techniques.


    \section{Error Analysis}
    From the results, we notice that our baseline consistently does the worst. This is sensible since KNN performance suffers from dimensionality. Our implementation of stochastic gradient descent attempts to further reduce our feature space by using elastic net. However, with our implementation of SGD we didn't normalize our features from -1 to 1; instead, they are a float representing the weight of a feature. Because SGD is sensitive to feature scaling, we believe that there is a possible intrinsic issue with the data. Additionally, SGD is generally trained on a much larger dataset. \\
    
    When looking at F1 scores, LSTMs have the highest F1 scores. We attribute this to the fact that there could be possible interactions between the genes that are accounted for within the LSTM. Additionally, since LSTMs are hard to train on such small datasets, it is sensible that the performance of the LSTM suffered when undersampling the dataset. However, we also note that in general when oversamlping the data, we get better results. This is sensible because we are trying to resolve the issue of class imbalance between the relapse and non-relapse data points. A good next step involves ensuring that our LSTM does not suffer from overfitting. Lastly, we'd like to point out that logistic regression still performs well when oversampling the data, which indicates that it is not always necessary to use complex models such as LSTMs. 
       
    \begin{thebibliography}{9}

\bibitem{obnibus}
Barrett, Tanya. "Gene Expression Omnibus (GEO)." \textit{The NCBI Handbook [Internet]}. 2nd Edition. U.S. National Library of Medicine, 2013. Web. 14 Dec. 2016.

\bibitem{cruz}
Cruz, Joseph A., and David S. Wishart. "Applications of Machine Learning in Cancer Prediction and Prognosis." \textit{Cancer Informatics} (2006): 59-77. \textit{US National Library of Medicine}. Web. 14 Dec. 2016.

\bibitem{kourou}
Kourou, Konstantina, Themis Exarchos, and Konstantinos Exarchos. "Machine Learning Applications in Cancer Prognosis and Prediction." \textit{Computational and Structural Biotechnology Journal} 13 (2015): 8-17. Web. 14 Dec. 2016.

\end{thebibliography}
    
%\end{multicols*}
\end{document}
